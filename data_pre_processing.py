# -*- coding: utf-8 -*-
"""Data pre-processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1QikY5xsJVWWxb5GukEFUlbwcfo8uHE

# Data Pre-processing techniques

**Dowload all the libraries - pandas, numpy, seaborn, matplotlib**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Load Titanic datset**"""

# Load dataset
df = pd.read_csv('/content/Titanic-Dataset.csv')

# Display the first few rows
df.head()

# see info. - we can see all the datatypes
df.info()

# see all column names
df.columns

#describe the data
df.describe()

"""**1. Data Cleaning**

*   Handling Missing Values
*   Outlier detection & removal
*   Dealing with duplicates

**Handling Missing Values**
"""

#check missing values
df.isnull().sum()

"""we can see we have 177 missing values in "Age", 687 missing values in "Cabin" and 2 missing values in "Embarked"."""

#Filling missing values - Age and Cabin
df['Age'].fillna(df['Age'].median(), inplace=True) #filling those values with age median value

df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True) #filling those values with mode

# Dropping Cabin column because it has too many missing values
if 'Cabin' in df.columns:
    df.drop(columns=['Cabin'], inplace=True)

#cheking the null values
df.isnull().sum()

"""**Outlier detection and removal**"""

#Handling ourliers
#Z-score - this approach is useful when data follows a normal distribution
#checking outliers in Age, Fare

from scipy import stats
import numpy as np

# Compute the z-scores
z_scores = np.abs(stats.zscore(df[['Age', 'Fare']]))  # Convert to absolute values

# Identify outliers where z-score > 3
outliers = df[(z_scores > 3).any(axis=1)]

print(f"Total Outliers: {outliers.shape[0]}")

# IQR - InterQuartileRange
import numpy as np

# Function to detect outliers using IQR
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]

# Check for outliers in Age and Fare
outliers_age = detect_outliers_iqr(df, "Age")
outliers_fare = detect_outliers_iqr(df, "Fare")

print(f"Outliers in Age: {outliers_age.shape[0]}, Outliers in Fare: {outliers_fare.shape[0]}")

"""Options for handling outliers:
* Remove them (if they are errors)
* Cap them (set extreme values to the 5th or 95th percentile)
* Transform them (apply log transformation)





"""

# Cap Fare outliers at 99th percentile
df["Fare"] = np.where(df["Fare"] > df["Fare"].quantile(0.99), df["Fare"].quantile(0.99), df["Fare"])

"""**Dealing with duplicates**"""

#cheking for duplicates
print(f"Duplicate Rows: {df.duplicated().sum()}")

#If we have duplicated and we want to remove them then use
df.drop_duplicates(inplace=True)

df



"""**2. Feature Scaling - Feature scaling ensures numerical values are within the same range, improving model performance.**

*  Normalization - Used when data is not normally distributed.
*  Standardization - Used when data is normally distributed.
*  Robust Scaling - When data has many outliers.

The Titanic dataset has a mix of numerical and categorical features.
* Among numerical features, Age and Fare are continuous variables, which usually benefit from scaling.
* Other numerical features like Pclass, SibSp, and Parch are discrete/categorical-like, so they don’t require scaling.
"""

#Normalization - (min-max scaling)
from sklearn.preprocessing import MinMaxScaler

normal = MinMaxScaler()
df[["Age", "Fare"]] = normal.fit_transform(df[["Age", "Fare"]])

df.head(5)

#Standardization - (Z-score scaling)
from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()
df[["Age", "Fare"]] = scalar.fit_transform(df[["Age", "Fare"]])

df.head(5)

from sklearn.preprocessing import RobustScaler

robust = RobustScaler()
df[["Age", "Fare"]] = robust.fit_transform(df[["Age", "Fare"]])

df.head(5)



"""**3. Encoding Categorical values - This is crutial because M.L models typically requires numerical inputs.**

* One-hot encoding - This method creates binary columns for each category.
* Label encoding - This technique assigns a unique integer to each category.
* Original encoding - Used for categorical variables that have a natural order.
* Binary encoding - combinaiton of one-hot and label encoding

In the Titanic dataset, the relevant categorical variables are:
Sex (male/female), Embarked (C, Q, S), Ticket_Prefix (if we extracted it)
"""

#one-hot encoding - This method creates binary columns for each category. It's suitable for nominal categorical variables like Sex and Embarked.

# One-Hot Encoding for 'Sex'
df = pd.get_dummies(df, columns=["Sex"], drop_first=True)

# One-Hot Encoding for 'Embarked'
df = pd.get_dummies(df, columns=["Embarked"], drop_first=True)

# Convert Boolean columns to integers (0 and 1)
df['Sex_male'] = df['Sex_male'].astype(int)
df['Embarked_Q'] = df['Embarked_Q'].astype(int)
df['Embarked_S'] = df['Embarked_S'].astype(int)

df.head(5)

"""Label encoding - This technique assigns a unique integer to each category. It’s useful for ordinal categorical variables, but since we don’t have any in this dataset, we won’t use it here.

Original encoding - Ordinal Encoding is used for categorical variables that have a natural order (ordinal categories). This method assigns an integer to each category based on its order.

Binary encoding - Binary Encoding is useful for high-cardinality categorical features (those with many unique categories). It first converts categories into integers, then converts those integers into binary code, and finally creates separate columns for each binary digit.
"""



"""**For ticket column, we have some options**
*1. Drop the ticket column*
*2. Extract useful information*
*3. Encoding the extracted information*
"""

# Extract useful information
# You could create two new features: Ticket_Prefix and Ticket_Number.
# Extract ticket prefix and number
df["Ticket_Prefix"] = df["Ticket"].apply(lambda x: x.split(" ")[0] if not x.isdigit() else "None")
df["Ticket_Number"] = df["Ticket"].apply(lambda x: x.split(" ")[-1] if not x.isdigit() else x)
df["Ticket_Number"] = pd.to_numeric(df["Ticket_Number"], errors="coerce")  # Convert to numeric

df.head(5)

# Encoding the Extracted Ticket Prefix using Label encoding
from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()
df['Ticket_Prefix'] = label.fit_transform(df["Ticket_Prefix"])

df.head(5)

#remove ticket from dataset
df = df.drop('Ticket', axis=1)

df.columns

"""**4. Feature Engineering**

* Feature Selection
* Dimensionality Reduction
* Feature extraction
* Binning

**Feature Selection** - Identify and keep the most relevant features while removing irrelevant or redundant ones. You can use various techniques for this, such as:

Correlation Analysis: Examine the correlation between features and the target variable (Survived) using correlation matrices.

Univariate Selection: Use statistical tests to select features based on their relationship with the target variable.

Tree-based Feature Importance: Fit a tree-based model (e.g., Random Forest) and use feature importance scores to select significant features.
"""

#correlation matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Create a correlation matrix
correlation_matrix = numeric_df.corr()

# Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

"""**Dimensionality Reduction**

If you have a large number of features, consider reducing the dimensionality of the dataset using techniques like:

Principal Component Analysis (PCA): Reduces the number of features while retaining most of the variance in the data.

Linear Discriminant Analysis (LDA): Focuses on maximizing class separability.

**Feature Extraction** - You can create new features based on existing ones, which may provide more insight.
"""

# Family Size: Combine SibSp (siblings/spouses aboard) and Parch (parents/children aboard) to create a Family_Size feature.

df["Family Size"] = df['SibSp'] + df['Parch'] + 1

df.head(5)

"""**Binning (Discretization)**- If you have continuous variables that you suspect might be better represented in discrete form, you can apply binning. For example, you could bin Fare into ranges (e.g., Low, Medium, High).

**5. Data Transformation - Data Transformation involves modifying the data to make it suitable for modeling. This can include normalization, standardization, and other techniques that help stabilize variance and make the data more interpretable for machine learning algorithms.**

* Log transformation - Log transformation is useful for reducing skewness in highly skewed distributions. You can apply it to columns like Fare if you find it to be positively skewed.
* Box - cox transformation - The Box-Cox transformation is a family of power transformations that stabilize variance and make data more normally distributed.
* power transformation (yeo-johnson) - The Yeo-Johnson transformation is similar to Box-Cox but can handle negative values as well.
"""

#Log transformation
import numpy as np

# Apply log transformation to 'Fare'
df['Fare'] = np.log1p(df['Fare'])  # Using log1p to handle zero values

df.head(5)

# Check for non-positive values in 'Fare'
non_positive_fares = df[df['Fare'] <= 0]
print(non_positive_fares)

#Box-cox transformation - fare has negative values, box cox can't handle negative values
from scipy import stats

# Apply Box-Cox transformation (only works on positive values)
df['Fare'], _ = stats.boxcox(df['Fare'] + 1)  # Shift by +1 to make all values positive

df.head(5)

#power transformation
# Define the numeric columns
numeric_cols = ['Age', 'Fare']
# Apply Yeo-Johnson transformation
from sklearn.preprocessing import PowerTransformer

# Create a PowerTransformer instance
pt = PowerTransformer(method='yeo-johnson')

# Apply Yeo-Johnson transformation
df[numeric_cols] = pt.fit_transform(df[numeric_cols])

df.head(5)

"""**6. Data Augmentation - Data augmentation is primarily used in scenarios where we want to artificially expand the dataset, such as in image processing, text processing, or imbalanced datasets.**

* **For Image Data** - Rotating, Flipping, Zooming, Cropping, etc...
* **Text Data** - Synonym replacement, random insertion, random deletion, random swapping of the words are great for NLP.
* **Numerical Data** - Synthetic Data Generation (oversampling and undersampling), Adding Gaussian Noise, Generating New Features.

*Since the Titanic dataset is structured (tabular) data, the common data augmentation techniques are synthetic data generation, oversampling, or adding noise.*
"""

#cheking how many zeros and once in survived column
df['Survived'].value_counts()

# Synthetic Data Generation (SMOTE) - Since Survived is a binary classification target, we can check for class imbalance and use SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class.
from imblearn.over_sampling import SMOTE

# Define features and target
X = df.drop(columns=['Survived'])  # Features
y = df['Survived']  # Target variable

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Convert back to DataFrame
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['Survived'] = y_resampled  # Add the target variable back

#Adding Gaussian Noise (for Numerical Features) - To slightly modify existing data without drastically changing its distribution, we can add Gaussian noise to numerical columns like Age and Fare.

# why?? Helps regularize the model and improve generalization.

import numpy as np

# Define noise level (small percentage of the standard deviation)
noise_level = 0.02

# Apply noise to numerical features
df['Age'] = df['Age'] + np.random.normal(0, noise_level * df['Age'].std(), df.shape[0])
df['Fare'] = df['Fare'] + np.random.normal(0, noise_level * df['Fare'].std(), df.shape[0])

df.head(5)

# Generating New Features (Feature Engineering as Augmentation) - If we don't want to artificially modify data, we can derive new features as augmentation.

# Example: Creating an "Alone" Feature
# Alone = 1 means the passenger had no family onboard.
# Alone = 0 means the passenger was with family.

df['Alone'] = (df['SibSp'] + df['Parch'] == 0).astype(int)

df.head(5)

# Now we can drop sibsp, parch columns
df = df.drop(columns = ['SibSp','Parch'])

df.head(5)



"""**7. Handling Imbalanced Data - If Output variable (target) is imbalanced, meaning one class significantly outweighs the other, we need to handle it to avoid biased model performance.**

* Resampling techniques - oversampling, undersampling
* Class weight adjustments

The target variable in the Titanic dataset is Survived (0 or 1), which represents whether a passenger survived or not. If this variable is imbalanced, meaning one class significantly outweighs the other, we need to handle it to avoid biased model performance.
"""

# Checking Class Distribution
import seaborn as sns
import matplotlib.pyplot as plt

# Print value counts
df["Survived"].value_counts().plot(kind="bar")
df['Survived'].value_counts()

print(X.isnull().sum())

df['Ticket_Number'].fillna(df['Ticket_Number'].median(), inplace=True)

df['Ticket_Number'].isnull().sum()

df.head(5)

# Oversampling (SMOTE) – Increasing Minority Class
from imblearn.over_sampling import SMOTE

# Define features and target
X = df.drop(columns=["Survived", "Name"])
y = df["Survived"]

# Apply SMOTE
smote = SMOTE(sampling_strategy="auto", random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check new class distribution
import pandas as pd
df = pd.DataFrame(X_resampled, columns=X.columns)
df["Survived"] = y_resampled
print(df["Survived"].value_counts(normalize=True))

# Undersampling – Reducing Majority Class - Instead of adding more data, we remove samples from the majority class.
from imblearn.under_sampling import RandomUnderSampler

# Apply undersampling
under_sampler = RandomUnderSampler(sampling_strategy="auto", random_state=42)
X_resampled, y_resampled = under_sampler.fit_resample(X, y)

# Check new class distribution
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled["Survived"] = y_resampled
print(df_resampled["Survived"].value_counts(normalize=True))

df['Survived'].value_counts()

# Class Weighting – Adjusting Model Weights - Some models allow us to assign higher weights to the minority class instead of modifying the dataset.
from sklearn.linear_model import LogisticRegression

# Define model with class_weight
model = LogisticRegression(class_weight="balanced", random_state=42)
model.fit(X, y)

# if you want to manually assign weights
model = LogisticRegression(class_weight={0:1, 1:10}, random_state=42)
model.fit(X, y)
# class 0 gets weight 1 and class 1 gets a weight of 10. Model pays 10x more attention to class 1



"""**8. Text Processing**

* Tokenization
* Lowercasing
* Removing Stopwords
* Stemming
* Lemmatization
* Removing Punctuations
* Numbers
* Special Charecters
* TF - IDF
"""

# Tokenization - Breaking text into words or sentences.
from nltk.tokenize import word_tokenize
text = "Braund, Mr. Owen Harris"
tokens = word_tokenize(text)
print(tokens)

# Lowercasing - Converting all characters to lowercase for consistency.
text = "Braund, Mr. Owen Harris"
text_lower = text.lower()
print(text_lower)

# Removing Stopwords - Removing common words like "the", "and", "is", etc.
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print(filtered_tokens)

# Stemming - Reducing words to their root form.
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
print(stemmed_words)

# Lemmatization - Converting words to their dictionary form (better than stemming).
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print(lemmatized_words)

# Removing Punctuation, Numbers, Special Characters
import re
text_clean = re.sub(r'[^a-zA-Z\s]', '', text_lower)  # Keep only letters and spaces
print(text_clean)

#TF-IDF (Term Frequency - Inverse Document Frequency) - Converts text into numerical features based on word importance.
from sklearn.feature_extraction.text import TfidfVectorizer
texts = ["Mr. Owen Harris", "Mrs. John Bradley", "Miss. Laina"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)
print(X.toarray())  # TF-IDF values



"""**9. Time-series processing (for time series data)**

* Resampling - Resampling involves changing the frequency of the time series data. For example, you can convert daily data into weekly or monthly data.
* Rolling Statistics - Rolling statistics involve calculating the mean or other statistics over a moving window. This is useful for smoothing the time series data and identifying trends.
* Differencing - Differencing is used to remove trends and seasonality from the time series data, making it stationary. This involves subtracting the previous observation from the current observation.

*The Titanic dataset is not a time series dataset since it does not have a time-based feature (like date or time). However, if you are working with time-series data in general, here’s how to perform common time-series preprocessing steps:*
"""

# Resmapling
import pandas as pd

# Example: Creating a sample time series DataFrame
date_rng = pd.date_range(start='2020-01-01', end='2020-01-10', freq='D')
data = pd.DataFrame(date_rng, columns=['date'])
data['data'] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
data.set_index('date', inplace=True)

# Resample to weekly frequency
weekly_data = data.resample('W').sum()  # or use .mean(), .min(), etc.
print(weekly_data)

# Rolling statistics
# Calculate rolling mean with a window of 3 days
data['rolling_mean'] = data['data'].rolling(window=3).mean()
print(data)

# Differencing
# Calculate the first difference
data['first_difference'] = data['data'].diff()
print(data)



"""**10. Data transformation - Data transformation techniques help to stabilize variance and make the data more normally distributed, which can improve model performance.**

* Log transformation - Log transformation is particularly useful for reducing skewness in highly skewed data. It can only be applied to positive values.
* Square root - Square root transformation is useful for reducing right skewness and can be applied to non-negative values.
* cube root transformation - Cube root transformation can handle both positive and negative values and is less aggressive than log transformation.

"""

# Log transformation
import numpy as np

# Assuming 'Fare' is highly skewed
df['Fare_log'] = np.log1p(df['Fare'])  # Using log1p to handle zero values gracefully

# Square transformation
# Apply square root transformation
df['Fare_sqrt'] = np.sqrt(df['Fare'])

# Apply cube root transformation
df['Fare_cbrt'] = np.cbrt(df['Fare'])

"""Summary of Transformations
* Log Transformation: Use when your data is strictly positive and highly skewed.

* Square Root Transformation: Use for count data or non-negative values that are moderately skewed.

* Cube Root Transformation: Useful when you have both positive and negative values and want to reduce skewness.
"""



"""**11. Handling sequence data - for NLP or any other time dependent data**

* Padding - Padding involves adding a specific value (usually zeros) to the sequences to ensure they all have the same length. This is especially important when using models like RNNs or LSTMs that require fixed-length input.


* Truncation - Truncation involves shortening sequences that are longer than a specified maximum length. This is useful to maintain a consistent input size and to prevent memory overflow.
"""

# Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Example sequences of varying lengths
sequences = [[1, 2, 3], [4, 5], [6]]
# Pad sequences to the maximum length (4 in this case)
padded_sequences = pad_sequences(sequences, padding='post')  # You can also use padding='pre'
print(padded_sequences)

"""Why Use Padding?

Ensures that all input sequences have the same length, which is required for batch processing in deep learning models.

Maintains the sequence information by padding at the start or end, as needed.

"""

# Truncation
# Example sequences of varying lengths
sequences = [[1, 2, 3, 4, 5], [6, 7], [8, 9, 10]]
# Pad sequences with truncation to maximum length of 3
truncated_padded_sequences = pad_sequences(sequences, maxlen=3, padding='post', truncating='post')
print(truncated_padded_sequences)

"""Why Use Truncation?

Helps to manage memory usage and computational efficiency by limiting the input size.

Ensures uniform input size without excessively increasing the training time or resources.

"""

